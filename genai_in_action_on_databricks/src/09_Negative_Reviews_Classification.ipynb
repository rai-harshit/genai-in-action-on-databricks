{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51b3d1c3-46e0-49c5-9b0b-08041b94221a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n",
      "Requirement already satisfied: openai in /databricks/python3/lib/python3.10/site-packages (0.28.1)\n",
      "Collecting openai\n",
      "  Downloading openai-1.54.3-py3-none-any.whl (389 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 389.6/389.6 kB 5.6 MB/s eta 0:00:00\n",
      "Collecting httpx<1,>=0.23.0\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.4/76.4 kB 6.4 MB/s eta 0:00:00\n",
      "Collecting jiter<1,>=0.4.0\n",
      "  Downloading jiter-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (327 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 327.5/327.5 kB 8.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /databricks/python3/lib/python3.10/site-packages (from openai) (1.10.6)\n",
      "Requirement already satisfied: tqdm>4 in /databricks/python3/lib/python3.10/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: sniffio in /databricks/python3/lib/python3.10/site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /databricks/python3/lib/python3.10/site-packages (from openai) (3.5.0)\n",
      "Collecting typing-extensions<5,>=4.11\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /databricks/python3/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Collecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.0/78.0 kB 8.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: certifi in /databricks/python3/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2022.12.7)\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 5.1 MB/s eta 0:00:00\n",
      "Installing collected packages: typing-extensions, jiter, h11, httpcore, httpx, openai\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Not uninstalling typing-extensions at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-a2519988-1bd2-4fd9-b7a1-19aa4d6b2873\n",
      "    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.28.1\n",
      "    Not uninstalling openai at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-a2519988-1bd2-4fd9-b7a1-19aa4d6b2873\n",
      "    Can't uninstall 'openai'. No files were found to uninstall.\n",
      "Successfully installed h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jiter-0.7.0 openai-1.54.3 typing-extensions-4.12.2\n",
      "\u001b[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "pip install -U openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "009dad04-07ad-44e4-914d-d2e2c252870f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "560ba44a-c340-47de-852a-45c999086371",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ce2cf9c-4526-4760-a263-6b97f9d9194d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### PERFORM TEXT TRANSLATION\n",
    "DATABRICKS_TOKEN = DATABRICKS_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "negative_review_classes = [\"Poor_Quality\", \"Poor_Customer_Experience\", \"Camera_Malfunction\", \"Product_Not_As_Described\", \"Shipping_Issues\", \"Design_Or_Build_Issues\",  \"Tripod_Issues\", \"Battery_Life_Issues\", \"Charging_Issues\", \"App_Or_Software_Issues\",\"Accessories_Issues\", \"Incompatibility_Issues\", \"Missing_Items\", \"Refund_Or_Return_Issues\",\"Packaging_Issues\", \"Other_Issues\"]\n",
    "def classify_negative_review(review):\n",
    "  # How to get your Databricks token: https://docs.databricks.com/en/dev-tools/auth/pat.html\n",
    "  # DATABRICKS_TOKEN = os.environ.get('DATABRICKS_TOKEN')\n",
    "  # Alternatively in a Databricks notebook you can use this:\n",
    "  client = OpenAI(\n",
    "    api_key=DATABRICKS_TOKEN,\n",
    "    base_url=\"<link-to-the-databricks-model-serving-endpoint>\"\n",
    "  )\n",
    "\n",
    "  chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": f'Classify the given negative review into one of the following categories: {negative_review_classes}. Your response must strictly be of the format: class1|class2|class3. Do not change or modify the class names. Here is the review: '\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": f\"{review}\"\n",
    "    }\n",
    "    ],\n",
    "    model=\"databricks-meta-llama-3-1-70b-instruct\",\n",
    "    max_tokens=600\n",
    "  )\n",
    "  return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc81ac5f-363b-4b6a-b10f-d3b39ab20b22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, split, col\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "# Register the translate_review function as a UDF\n",
    "classify_negative_review_udf = udf(classify_negative_review, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "569b7698-b12b-4da8-9aca-b3ccb921070a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sentiment = spark.read.table(\"harshit_rai_genai_demo.products.camera_reviews_with_sentiment_analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e498b394-59cc-492f-847a-1707ddb71e73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_negative = df_sentiment.filter(df_sentiment.rating <= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da02aab6-bf86-4f13-a5d0-58464f9b0d20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_classification = df_negative.select(\"review_id\", \"translated_review\",\"rating\",\"sentiment\", \"summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bac25066-7fef-4241-a76d-345d54d5dfcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_classification = df_classification.withColumn(\"issue_type\", classify_negative_review_udf(col(\"summary\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a016065-9620-4890-82ea-9c3aebcd988c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, when, col, array_contains\n",
    "split_col = split(col(\"issue_type\"), \"\\|\")\n",
    "for cls in negative_review_classes:\n",
    "    df_classification = df_classification.withColumn(cls, when(array_contains(split_col, cls), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3378e10e-e645-4ab7-bfaa-380af22eca33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_classification.write.mode('overwrite').saveAsTable(\"harshit_rai_genai_demo.products.issue_type_classification\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Negative_Reviews_Classification",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
